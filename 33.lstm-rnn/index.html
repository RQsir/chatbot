



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-0.17.2, mkdocs-material-2.2.5">
    
    
      
        <title>三十三-LSTM-RNN—有记忆的神经网络 - 自己动手做聊天机器人</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.bcabdff3.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.792431c1.css">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
    
    
    
      <body data-md-color-primary="indigo" data-md-color-accent="indigo">
    
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="自己动手做聊天机器人" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                自己动手做聊天机器人
              </span>
              <span class="md-header-nav__topic">
                三十三-LSTM-RNN—有记忆的神经网络
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">&#xE5CD;</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <span class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </span>
    自己动手做聊天机器人
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="概述" class="md-nav__link">
      概述
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../1.knowledge/" title="一-涉及知识" class="md-nav__link">
      一-涉及知识
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../2.nltk/" title="二-初识NLTK库" class="md-nav__link">
      二-初识NLTK库
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../3.corpus/" title="三-语料与词汇资源" class="md-nav__link">
      三-语料与词汇资源
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../4.pos-tagging/" title="四-自动化词性标注" class="md-nav__link">
      四-自动化词性标注
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../5.text-classification/" title="五-文本分类" class="md-nav__link">
      五-文本分类
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../6.extract/" title="六-结构化提取" class="md-nav__link">
      六-结构化提取
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../7.grammar/" title="七-文法分析" class="md-nav__link">
      七-文法分析
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../8.nlp-review/" title="八-自然语言处理" class="md-nav__link">
      八-自然语言处理
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../9.chatbot/" title="九-聊天机器人怎么做" class="md-nav__link">
      九-聊天机器人怎么做
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../10.pos-ke/" title="十-词性标注与关键词提取" class="md-nav__link">
      十-词性标注与关键词提取
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../11.storage/" title="十一-0字节存储海量语料资源" class="md-nav__link">
      十一-0字节存储海量语料资源
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../12.semantic-dependency/" title="十二-用语言云分析依存句法和语义依存" class="md-nav__link">
      十二-用语言云分析依存句法和语义依存
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../13.linguistic-model/" title="十三-探究语言模型" class="md-nav__link">
      十三-探究语言模型
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../14.participles-art/" title="十四-探究中文分词" class="md-nav__link">
      十四-探究中文分词
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../15.probabilistic-model/" title="十五-概率图模型" class="md-nav__link">
      十五-概率图模型
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../16.bag-material/" title="十六-自然语言处理中的囊中取物" class="md-nav__link">
      十六-自然语言处理中的囊中取物
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../17.specific-mehtod/" title="十七-词性自动标注" class="md-nav__link">
      十七-词性自动标注
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../18.tree/" title="十八-生成句法分析树" class="md-nav__link">
      十八-生成句法分析树
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../19.understand/" title="十九-机器人是怎么理解“日后再说”的" class="md-nav__link">
      十九-机器人是怎么理解“日后再说”的
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../20.pos-base-method/" title="二十-语义角色标注" class="md-nav__link">
      二十-语义角色标注
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../21.tf-idf/" title="二十一-隐含语义索引模型" class="md-nav__link">
      二十一-隐含语义索引模型
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../22.artificial-neural-network/" title="二十二-人工神经网络" class="md-nav__link">
      二十二-人工神经网络
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../23.cnn/" title="二十三-用CNN做深度学习" class="md-nav__link">
      二十三-用CNN做深度学习
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../24.nlp-deep-learning/" title="二十四-将深度学习应用到NLP" class="md-nav__link">
      二十四-将深度学习应用到NLP
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../25.word2vec/" title="二十五-word2vec的实现原理" class="md-nav__link">
      二十五-word2vec的实现原理
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../26.rnn/" title="二十六-递归神经网络(RNN)" class="md-nav__link">
      二十六-递归神经网络(RNN)
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../27.auto-faq/" title="二十七-自动问答" class="md-nav__link">
      二十七-自动问答
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../28.subtitle-corpus/" title="二十八-基于美剧字幕的聊天语料库建设" class="md-nav__link">
      二十八-基于美剧字幕的聊天语料库建设
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../29.corpus-1g/" title="二十九-近1GB的三千万聊天语料" class="md-nav__link">
      二十九-近1GB的三千万聊天语料
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../30.tiny-rabbit/" title="三十-第一版聊天机器人小二兔" class="md-nav__link">
      三十-第一版聊天机器人小二兔
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../31.directive/" title="三十一-把网站流量导向小二兔" class="md-nav__link">
      三十一-把网站流量导向小二兔
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../32.word-vector/" title="三十二-用影视剧字幕语料库生成词向量" class="md-nav__link">
      三十二-用影视剧字幕语料库生成词向量
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        三十三-LSTM-RNN—有记忆的神经网络
      </label>
    
    <a href="./" title="三十三-LSTM-RNN—有记忆的神经网络" class="md-nav__link md-nav__link--active">
      三十三-LSTM-RNN—有记忆的神经网络
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lstm" title="掌握LSTM有什么用" class="md-nav__link">
    掌握LSTM有什么用
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#100pythonlstm" title="100多行原始python代码实现的基于LSTM的二进制加法器" class="md-nav__link">
    100多行原始python代码实现的基于LSTM的二进制加法器
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lstmpython" title="另一个完整的LSTM的python实现" class="md-nav__link">
    另一个完整的LSTM的python实现
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lstm_1" title="利用lstm在输入一串连续质数时预估下一个质数" class="md-nav__link">
    利用lstm在输入一串连续质数时预估下一个质数
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../34.torch/" title="三十四-深度学习框架torch" class="md-nav__link">
      三十四-深度学习框架torch
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../35.lstm-learning/" title="三十五-一学会甄嬛体" class="md-nav__link">
      三十五-一学会甄嬛体
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../36.tensorflow-session-graph/" title="三十六-tensorflow的session和graph" class="md-nav__link">
      三十六-tensorflow的session和graph
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../37.tensorflow-lr/" title="三十七-tensorflow中的线性回归" class="md-nav__link">
      三十七-tensorflow中的线性回归
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../38.make-chatbot/" title="三十八-聊天机器人构建流程" class="md-nav__link">
      三十八-聊天机器人构建流程
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../39.gpu-server/" title="三十九-搭建一台GPU共享云服务" class="md-nav__link">
      三十九-搭建一台GPU共享云服务
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../40.video-knowledge-point/" title="四十-视频之开篇宣言与知识点" class="md-nav__link">
      四十-视频之开篇宣言与知识点
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../41.video-python/" title="四十一-视频之环境搭建与python基础" class="md-nav__link">
      四十一-视频之环境搭建与python基础
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../42.practice/" title="四十二-从理论到实践开发聊天机器人1" class="md-nav__link">
      四十二-从理论到实践开发聊天机器人1
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../43.practice2/" title="四十三-从理论到实践开发聊天机器人2" class="md-nav__link">
      四十三-从理论到实践开发聊天机器人2
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lstm" title="掌握LSTM有什么用" class="md-nav__link">
    掌握LSTM有什么用
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#100pythonlstm" title="100多行原始python代码实现的基于LSTM的二进制加法器" class="md-nav__link">
    100多行原始python代码实现的基于LSTM的二进制加法器
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lstmpython" title="另一个完整的LSTM的python实现" class="md-nav__link">
    另一个完整的LSTM的python实现
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lstm_1" title="利用lstm在输入一串连续质数时预估下一个质数" class="md-nav__link">
    利用lstm在输入一串连续质数时预估下一个质数
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>三十三-LSTM-RNN—有记忆的神经网络</h1>
                
                <p>不管是什么样的神经网络算法，总能找到各种语言的库帮助你实现你的需求，但是我们总是看不清那个黑盒子里到底是怎样的一种存在，如果想真正掌握一种算法，最实际的方法就是完全手写出来，本节我就通过两套LSTM的简单实现来分享掌握LSTM的快乐 </p>
<h3 id="lstm">掌握LSTM有什么用<a class="headerlink" href="#lstm" title="Permanent link">&para;</a></h3>
<p>在读代码之前我们要先知道我们读这段代码的意义是什么。LSTM（Long Short Tem Memory）是一种特殊递归神经网络，它的特殊性在于它的神经元的设计能够保存历史记忆，这样可以解决自然语言处理的统计方法中只能考虑最近n个词语而忽略了更久之前的词语的问题。它的用途有：word representation（embedding）(也就是怎么样把词语表达成向量)、sequence to sequence learning（也就是输入一个句子能预测另一个句子）、以及机器翻译、语音识别等</p>
<h3 id="100pythonlstm">100多行原始python代码实现的基于LSTM的二进制加法器<a class="headerlink" href="#100pythonlstm" title="Permanent link">&para;</a></h3>
<p>这份代码来源于<a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">anyone-can-code-lstm</a>，国人也做了<a href="http://blog.csdn.net/zzukun/article/details/49968129">翻译</a>，这里我再次引用意在一边读一边学习一边理解，并从制作聊天机器人的目标上做一些扩充解读，下面我就一句一句的解释（没有缩进，请参考原始代码）：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span><span class="o">,</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>最开始引入了numpy库，这是为了矩阵操作的方便</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</td></tr></table>

<p>这是声明sigmoid激活函数，这是神经网络的基础内容，常用的激活函数有sigmoid、tan、relu等，sigmoid的取值范围是[0, 1]，tan的取值范围是[-1,1]，这里的x是一个向量，那么返回的output也是一个向量</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_output_to_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">output</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这是声明sigmoid的求导函数，那么这个求导函数怎么来的呢？请看下面的推导你就明白了
<img alt="" src="../imgs/8780926a58ca7c4acd6c0316542e0ea94a688519.png" /></p>
<p>在这里先说一下这个加法器的思路：二进制的加法是一个一个二进制位相加，同时会记录一个满二进一的进位，那么训练时，随机找个c=a+b就是一个样本，输入a、b输出c就是整个lstm的预测过程，我们要训练的就是由a、b的二进制向c转换的各种转换矩阵和权重等，也就是我们要设计的神经网络</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">int2binary</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</td></tr></table>

<p>这里声明了一个词典，用于由整型数字转成二进制，这样存起来是为了不用随时计算，提前存好能够使得读取更快</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">binary_dim</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">largest_number</span> <span class="o">=</span> <span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">binary_dim</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这是声明二进制数字的维度，如果是8，那么二进制能表达的最大整数是2^8=256，也就是这里的largest_number</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unpackbits</span><span class="p">(</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">range</span><span class="p">(</span><span class="n">largest_number</span><span class="p">)],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">largest_number</span><span class="p">):</span>
    <span class="n">int2binary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">binary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<p>这就是所说的预先把整数到二进制的转换词典存起来</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</td></tr></table>

<p>这里设置了几个参数，alpha是学习速度，input_dim是输入层向量的维度，因为输入a、b两个数，所以是2，hidden_dim是隐藏层向量的维度，也就是隐藏层神经元的个数，output_dim是输出层向量的维度，因为输出一个c，所以是1维。这样从输入层到隐藏层的权重矩阵一定是2<em>16维的，从隐藏层到输出层的权重矩阵一定是16</em>1维的，而隐藏层到隐藏层的权重矩阵一定是16*16维的，也就是如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_0</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">synapse_1</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">output_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">synapse_h</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</td></tr></table>

<p>这里用2x-1是因为np.random.random会生成从0到1之间的随机浮点数，那么2x-1就是使其取值范围在[-1, 1]</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_0_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">synapse_0</span><span class="p">)</span>
<span class="n">synapse_1_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">synapse_1</span><span class="p">)</span>
<span class="n">synapse_h_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">synapse_h</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这是声明了三个矩阵的更新，也就是δ，后面会用到</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>for j in range(10000):
</pre></div>
</td></tr></table>

<p>这表示我们要对下面的过程进行10000次迭代</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">a_int</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">largest_number</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">int2binary</span><span class="p">[</span><span class="n">a_int</span><span class="p">]</span>
<span class="n">b_int</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">largest_number</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">int2binary</span><span class="p">[</span><span class="n">b_int</span><span class="p">]</span>
<span class="n">c_int</span> <span class="o">=</span> <span class="n">a_int</span> <span class="o">+</span> <span class="n">b_int</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">int2binary</span><span class="p">[</span><span class="n">c_int</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<p>这里其实是随机生成一个样本，这个样本包含二进制的a、b、c，其中c=a+b，a_int、b_int、c_int分别是是a、b、c对应的整数格式</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这个d在后面用来存我们模型对c的预测值</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">overallError</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</td></tr></table>

<p>这个是全局误差，用来观察模型效果</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_2_deltas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<p>这用来存储第二层(也就是输出层)的残差，对于输出层，残差计算公式推导如下（公式可以在<a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95">这里</a>找到）：
<img alt="" src="../imgs/baded62b2a3bfd77b300b060087500a89f12abab.png" /></p>
<p>这个公式下面会用到</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_1_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">layer_1_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这用来存储第一层(也就是隐藏层)的输出值，首先我们赋0值作为上一个时间的值</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">binary_dim</span><span class="p">):</span>
</pre></div>
</td></tr></table>

<p>这里我们来遍历二进制的每一位</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">a</span><span class="p">[</span><span class="n">binary_dim</span> <span class="o">-</span> <span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span><span class="n">b</span><span class="p">[</span><span class="n">binary_dim</span> <span class="o">-</span> <span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">c</span><span class="p">[</span><span class="n">binary_dim</span> <span class="o">-</span> <span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</td></tr></table>

<p>这里的X和y分别是样本的输入和输出的二进制值的第position位，其中X对于每个样本有两个值，分别是a和b对应的第position位。把样本拆成每个二进制位用于训练是因为二进制加法中存在进位标记正好适合利用LSTM的长短期记忆来训练，每个样本的8个二进制位刚好是一个时间序列</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">synapse_0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">synapse_h</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这里使用的公式是Ct = σ(W0·Xt + Wh·Ct-1)</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1</span><span class="p">,</span><span class="n">synapse_1</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这里使用的公式是C2 = σ(W1·C1)</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_2_error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">layer_2</span>
</pre></div>
</td></tr></table>

<p>这里计算预测值和真实值的误差</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_2_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">layer_2_error</span><span class="p">)</span><span class="o">*</span><span class="n">sigmoid_output_to_derivative</span><span class="p">(</span><span class="n">layer_2</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这里开始反向传导的过程，通过如下公式计算delta(上面提到的公式用在这里)，并添加到数组layer_2_deltas中，此数组对于每个二进制位(position)有一个值</p>
<p><img alt="" src="../imgs/5efdad7ad8aeeee394eb2ce5a058582ff4533c61.png" /></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">overallError</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">layer_2_error</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</td></tr></table>

<p>这里计算累加总误差，后面用于展示和观察</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">d</span><span class="p">[</span><span class="n">binary_dim</span> <span class="o">-</span> <span class="n">position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">layer_2</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</td></tr></table>

<p>这里存储预测出来的position位的输出值</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_1_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">layer_1</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这里存储中间过程生成的隐藏层的值</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">future_layer_1_delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这用来存储用于下一个时间周期用到的隐藏层的历史记忆值，还是先赋一个空值</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">binary_dim</span><span class="p">):</span>
</pre></div>
</td></tr></table>

<p>这里我们再次遍历二进制的每一位</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">a</span><span class="p">[</span><span class="n">position</span><span class="p">],</span><span class="n">b</span><span class="p">[</span><span class="n">position</span><span class="p">]]])</span>
</pre></div>
</td></tr></table>

<p>和前面一样取出X的值，不同的是我们从大位开始做更新，因为反向传导是按时序逆着一级一级更新的</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">layer_1_values</span><span class="p">[</span><span class="o">-</span><span class="n">position</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<p>取出这一位对应隐藏层的输出</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">prev_layer_1</span> <span class="o">=</span> <span class="n">layer_1_values</span><span class="p">[</span><span class="o">-</span><span class="n">position</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<p>取出这一位对应隐藏层的上一时序的输出</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_2_delta</span> <span class="o">=</span> <span class="n">layer_2_deltas</span><span class="p">[</span><span class="o">-</span><span class="n">position</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</td></tr></table>

<p>取出这一位对应输出层的delta</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">layer_1_delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">future_layer_1_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">synapse_h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">layer_2_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">synapse_1</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="o">*</span> <span class="n">sigmoid_output_to_derivative</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里其实是基于下面的神经网络反向传导公式，并额外加上了隐藏层的δ值得出的</p>
<p><img alt="" src="../imgs/d1ed3d1dd569961b2541d89fe1675f95b1ddbb12.png" /></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_1_update</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_2_delta</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里累加权重矩阵的更新，基于下面的公式，也就是对权重(权重矩阵)的偏导等于本层的输出与下一层的delta的点乘</p>
<p><img alt="" src="../imgs/2060000a147f56f518d812c48b225e0932ffd294.png" /></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_h_update</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">prev_layer_1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>对前一时序的隐藏层权重矩阵的更新和上面公式类似，只不过改成前一时序的隐藏层输出与本时序的delta的点乘</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_0_update</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">layer_1_delta</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>对输入层权重矩阵的更新也是类似</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">future_layer_1_delta</span> <span class="o">=</span> <span class="n">layer_1_delta</span>
</pre></div>
</td></tr></table>

<p>记录下本时序的隐藏层的delta用来在下一时序使用</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_0</span> <span class="o">+=</span> <span class="n">synapse_0_update</span> <span class="o">*</span> <span class="n">alpha</span>
<span class="n">synapse_1</span> <span class="o">+=</span> <span class="n">synapse_1_update</span> <span class="o">*</span> <span class="n">alpha</span>
<span class="n">synapse_h</span> <span class="o">+=</span> <span class="n">synapse_h_update</span> <span class="o">*</span> <span class="n">alpha</span>
</pre></div>
</td></tr></table>

<p>对权重矩阵做更新</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">synapse_0_update</span> <span class="o">*=</span> <span class="mi">0</span>
<span class="n">synapse_1_update</span> <span class="o">*=</span> <span class="mi">0</span>
<span class="n">synapse_h_update</span> <span class="o">*=</span> <span class="mi">0</span>
</pre></div>
</td></tr></table>

<p>更新变量归零</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">if</span><span class="p">(</span><span class="n">j</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">print</span> <span class="s2">&quot;Error:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">overallError</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;Pred:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;True:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">d</span><span class="p">)):</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="n">x</span><span class="o">*</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
        <span class="k">print</span> <span class="nb">str</span><span class="p">(</span><span class="n">a_int</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; + &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">b_int</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;------------&quot;</span>
</pre></div>
</td></tr></table>

<p>这里在每训练了1000个样本后输出总误差信息，运行时可以看到收敛的过程</p>
<p>这套代码可以说是LSTM最简单的实现，没有考虑偏置变量，只有两个神经元，因此可以作为一个demo来学习，相比来说下面这个LSTM就是一个比较完善的实现了，几乎是按照论文great intro paper描述的完整实现</p>
<h3 id="lstmpython">另一个完整的LSTM的python实现<a class="headerlink" href="#lstmpython" title="Permanent link">&para;</a></h3>
<p>这个实现比上一个更符合LSTM，完全参照论文great intro paper来实现的，不到200行代码，原始代码来源于<a href="https://github.com/nicodjimenez/lstm">nicodjimenez</a>，作者解释在<a href="http://nicodjimenez.github.io/2014/08/08/lstm.html">这里</a>，具体过程还可以参考<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">2015-08-Understanding-LSTMs</a>中的图来理解。下面我就一句一句的解释（具体缩进请参考原始代码）：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>这里同样是声明sigmoid函数</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">rand_arr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">a</span>
</pre></div>
</td></tr></table>

<p>用于生成随机矩阵，取值范围是[a,b)，shape用args指定</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LstmParam</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mem_cell_ct</span> <span class="o">=</span> <span class="n">mem_cell_ct</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_dim</span> <span class="o">=</span> <span class="n">x_dim</span>
        <span class="n">concat_len</span> <span class="o">=</span> <span class="n">x_dim</span> <span class="o">+</span> <span class="n">mem_cell_ct</span>
        <span class="c1"># weight matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wg</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">)</span>
        <span class="c1"># bias terms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bg</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="n">rand_arr</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="c1"># diffs (derivative of loss function w.r.t. all parameters)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wg_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">concat_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bg_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>LstmParam类用于传递相关参数，其中mem_cell_ct是lstm的神经元数目，x_dim是输入数据的维度，concat_len是mem_cell_ct与x_dim的长度和，wg是输入节点的权重矩阵(这里的g不要理解为gate，原始论文里有解释)，wi是输入门的权重矩阵，wf是忘记门的权重矩阵，wo是输出门的权重矩阵，bg、bi、bf、bo分别是输入节点、输入门、忘记门、输出门的偏置，wg_diff、wi_diff、wf_diff、wo_diff分别是输入节点、输入门、忘记门、输出门的权重损失，bg_diff、bi_diff、bf_diff、bo_diff分别是输入节点、输入门、忘记门、输出门的偏置损失，这里初始化时会按照矩阵维度初始化，并把损失矩阵归零</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">apply_diff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wg</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">wg_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bg</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bg_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_diff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_diff</span>
        <span class="c1"># reset diffs to zero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wg_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bg_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo_diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里定义了权重的更新过程，先减掉损失，再把损失矩阵归零</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LstmState</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottom_diff_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottom_diff_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottom_diff_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x_dim</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>LstmState用于存储LSTM神经元的状态，这里包括g、i、f、o、s、h，其中s是内部状态矩阵(可以理解为记忆)，h是隐藏层神经元的输出矩阵</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LstmNode</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_param</span><span class="p">,</span> <span class="n">lstm_state</span><span class="p">):</span>
        <span class="c1"># store reference to parameters and to activations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">lstm_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">lstm_param</span>
        <span class="c1"># non-recurrent input to node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># non-recurrent input concatenated with recurrent input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xc</span> <span class="o">=</span> <span class="bp">None</span>
</pre></div>
</td></tr></table>

<p>一个LstmNode对应一个样本的输入，其中x就是输入样本的x，xc是用hstack把x和递归的输入节点拼接出来的矩阵（hstack是横着拼成矩阵，vstack就是纵着拼成矩阵）</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">bottom_data_is</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">h_prev</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="c1"># if this is the first lstm node in the network</span>
        <span class="k">if</span> <span class="n">s_prev</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span> <span class="n">s_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">h_prev</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span> <span class="n">h_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">)</span>
        <span class="c1"># save data for use in backprop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_prev</span> <span class="o">=</span> <span class="n">s_prev</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_prev</span> <span class="o">=</span> <span class="n">h_prev</span>

        <span class="c1"># concatenate x(t) and h(t-1)</span>
        <span class="n">xc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span>  <span class="n">h_prev</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wg</span><span class="p">,</span> <span class="n">xc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wi</span><span class="p">,</span> <span class="n">xc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wf</span><span class="p">,</span> <span class="n">xc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wo</span><span class="p">,</span> <span class="n">xc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">+</span> <span class="n">s_prev</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xc</span> <span class="o">=</span> <span class="n">xc</span>
</pre></div>
</td></tr></table>

<p>这里的bottom和后面遇到的top分别是两个方向，输入样本时认为是从底部输入，反向传导时认为是从顶部开始向底部传导，这里的bottom_data_is是输入样本的过程，首先把x和先前的输入拼接成矩阵，然后用公式wx+b分别计算g、i、f、o的值的值，这里面的激活函数有tanh和sigmoid，实际上可以看下面的图来理解，先说代码里g和i的计算的示意图：</p>
<p><img alt="" src="../imgs/b63ca1f0b02e39acc998f7a24dd9d8faa2b8f325.png" /></p>
<p>这里的[ht-1,xt]其实就是np.hstack((x,  h_prev))，这里的Ct其实就是g</p>
<p>再看代码里f的计算示意图：
<img alt="" src="../imgs/7b75509ae30fa66125feda9e9708597521c67fbb.png" />
再看代码里s的计算示意图（s对应的是图里的C）：
<img alt="" src="../imgs/40d320f4287767813f13388169ad7421de8d4d17.png" />
再看代码里o和h的计算示意图(这里的h的计算公式和代码里有不同，不确定作者为什么要修改这一部分，我们可以尝试按照论文来修改看效果哪个更好)：
<img alt="" src="../imgs/c8e476b01eb6acb352a102a6d060131559ada55d.png" />
所以其实整个就是这样一个过程：
<img alt="" src="../imgs/82117fcc91f4c8f1b357ee216c2fc78f91efd50d.png" /></p>
<p>每个时序的神经网络可以理解为有四个神经网络层(图中黄色的激活函数部分)，最左边的是忘记门，它直接生效到记忆C上，第二个是输入门，它主要依赖于输入的样本数据，之后按照一定“比例”影响记忆C，这里的“比例”是通过第三个层(tanh)来实现的，因为它取值范围是[-1,1]可以是正向影响也可以是负向影响，最后一个是输出门，每一时序产生的输出既依赖于输入的样本x和上一时序的输出，还依赖于记忆C，整个设计几乎模仿了生物神经元的记忆功能，应该容易理解。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">top_diff_is</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_diff_h</span><span class="p">,</span> <span class="n">top_diff_s</span><span class="p">):</span>
        <span class="c1"># notice that top_diff_s is carried along the constant error carousel</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span> <span class="o">*</span> <span class="n">top_diff_h</span> <span class="o">+</span> <span class="n">top_diff_s</span>
        <span class="n">do</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span> <span class="o">*</span> <span class="n">top_diff_h</span>
        <span class="n">di</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">*</span> <span class="n">ds</span>
        <span class="n">dg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">*</span> <span class="n">ds</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_prev</span> <span class="o">*</span> <span class="n">ds</span>

        <span class="c1"># diffs w.r.t. vector inside sigma / tanh function</span>
        <span class="n">di_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">*</span> <span class="n">di</span>
        <span class="n">df_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span> <span class="o">*</span> <span class="n">df</span>
        <span class="n">do_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span>
        <span class="n">dg_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dg</span>

        <span class="c1"># diffs w.r.t. inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wi_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">di_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wf_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">df_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wo_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">do_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wg_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">dg_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bi_diff</span> <span class="o">+=</span> <span class="n">di_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bf_diff</span> <span class="o">+=</span> <span class="n">df_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bo_diff</span> <span class="o">+=</span> <span class="n">do_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bg_diff</span> <span class="o">+=</span> <span class="n">dg_input</span>

        <span class="c1"># compute bottom diff</span>
        <span class="n">dxc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
        <span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wi</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">di_input</span><span class="p">)</span>
        <span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wf</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">df_input</span><span class="p">)</span>
        <span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wo</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">do_input</span><span class="p">)</span>
        <span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wg</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dg_input</span><span class="p">)</span>

        <span class="c1"># save bottom diffs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_s</span> <span class="o">=</span> <span class="n">ds</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_x</span> <span class="o">=</span> <span class="n">dxc</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">x_dim</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_h</span> <span class="o">=</span> <span class="n">dxc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">x_dim</span><span class="p">:]</span>
</pre></div>
</td></tr></table>

<p>这里是反向传导的过程，这个过程是整个训练过程的核心，如果不了解原理，这部分是不可能理解的，所以我们首先了解一下公式推导：</p>
<p>假设在t时刻lstm输出的预测值为h(t)，而实际的输出值是y(t)，那么他们之间的差别就是损失，我们假设损失函数为l(t) = f(h(t), y(t)) = ||h(t) - y(t)||^2，也就是欧式距离，那么整体损失函数就是</p>
<p>L(t) = ∑l(t)，其中t从1到T，T表示整个事件序列的最大长度</p>
<p>我们的最终目标就是用梯度下降法来让L(t)最小化，也就是找到一个最优的权重w使得L(t)最小，那么权重w在什么情况下L(t)最小呢？就是在：当w发生微小变化时L(t)不再变化，也就是达到局部最优，即L对w的偏导也就是梯度为0</p>
<p>下面我们来分析L对w的微分怎么算</p>
<p><img alt="" src="../imgs/6792a46c8313018b2110de2f1e1535c2b12d0118.png" /></p>
<p>这个公式这样来理解：dL/dw表示当w发生单位变化时L变化了多少，dh(t)/dw表示当w发生单位变化时h(t)变化了多少，dL/dh(t)表示当h(t)发生单位变化时L变化了多少，那么(dL/dh(t)) * (dh(t)/dw)就表示对于第t时序第i个记忆单元当w发生单位变化时L变化了多少，那么把所有由1到M的i和所有由1到T的t累加起来自然就是整体上的dL/dw</p>
<p>下面单独来看左边的dL/dh(t)，它其实可以写成</p>
<p><img alt="" src="../imgs/189af1af9cd9dca649505c83f0e472599e4af813.png" /></p>
<p>右面表示：对于第i个记忆单元，当h(t)发生单位变化时，整个从1到T的时序上所有局部损失l的累加和，这其实刚好就是dL/dh(t)，那么实际上h(t)只会影响从t到T这段时序上的局部损失l，所以式子可以写成</p>
<p><img alt="" src="../imgs/6534f9f4ec1a66a450d87e49de0c4e14638bce68.png" /></p>
<p>下面我们假设L(t)表示从t到T的损失和，那么有L(t) = ∑l(s)，其中s由t到T，那么我们上面的式子可以写成</p>
<p><img alt="" src="../imgs/2414505afc5c9ce64e6eb3b704ef0ca7dadda133.png" /></p>
<p>那么我们上面的梯度公式就可以写成</p>
<p><img alt="" src="../imgs/da204d7c27dce459bfdb92c9ea74fc1962a9d4ed.png" /></p>
<p>右边其实就是h(t)对w的导数，下面我们来专门看左边的式子</p>
<p>我们知道L(t) = l(t) + L(t+1)，那么dL(t)/dh(t) = dl(t)/dh(t) + dL(t+1)/dh(t)，右边的式子其实就是LSTM带有记忆的真谛之所在，也就是用下一时序的导数可以得出当前时序的导数，那么我们就按照这个规律来做推导，首先我们计算T时刻的导数然后往前推，在T时刻，dL(T)/dh(T) = dl(T)/dh(T)</p>
<p>为了方便理解，我们暂时先略过上面那段top_diff_is，看之后的一段</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LstmNetwork</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_param</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_param</span> <span class="o">=</span> <span class="n">lstm_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># input sequence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">y_list_is</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_list</span><span class="p">,</span> <span class="n">loss_layer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates diffs by setting target sequence</span>
<span class="sd">        with corresponding loss layer.</span>
<span class="sd">        Will *NOT* update parameters.  To update parameters,</span>
<span class="sd">        call self.lstm_param.apply_diff()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_list</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_list</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_list</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># first node only gets diffs from label ...</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_layer</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">y_list</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">diff_h</span> <span class="o">=</span> <span class="n">loss_layer</span><span class="o">.</span><span class="n">bottom_diff</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">y_list</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="c1"># here s is not affecting loss due to h(t+1), hence we set equal to zero</span>
        <span class="n">diff_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_param</span><span class="o">.</span><span class="n">mem_cell_ct</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">top_diff_is</span><span class="p">(</span><span class="n">diff_h</span><span class="p">,</span> <span class="n">diff_s</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="c1">### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h</span>
        <span class="c1">### we also propagate error along constant error carousel using diff_s</span>
        <span class="k">while</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_layer</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">y_list</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">diff_h</span> <span class="o">=</span> <span class="n">loss_layer</span><span class="o">.</span><span class="n">bottom_diff</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">y_list</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="n">diff_h</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_h</span>
            <span class="n">diff_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_s</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">top_diff_is</span><span class="p">(</span><span class="n">diff_h</span><span class="p">,</span> <span class="n">diff_s</span><span class="p">)</span>
            <span class="n">idx</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</td></tr></table>

<p>重点关注这里的diff_h(表达的是预测结果误差发生单位变化时损失L是多少，也就相当于公式中的dL(t)/dh(t)的一个数值计算)，我们看到这里的过程是由idx从T往前遍历到1，计算loss_layer.bottom_diff和下一个时序的bottom_diff_h的和作为diff_h(其中第一次遍历即T时不加bottom_diff_h和公式一样)</p>
<p>其中loss_layer.bottom_diff的实现如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">bottom_diff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">diff</span>
</pre></div>
</td></tr></table>

<p>这里求的就是l(t) = f(h(t), y(t)) = ||h(t) - y(t)||^2的导数l'(t) = 2 * (h(t) - y(t))</p>
<p>与上面推导dL(t)/dh(t) 的过程类似，我们来推导dL(t)/ds(t) 。当s(t)发生变化时，L(t)的变化来源于s(t)影响了h(t)和h(t+1)，从而影响了L(t)，即</p>
<p><img alt="" src="../imgs/1ef37b37a86867961db65047e8f26873e16a5657.png" /></p>
<p>因为h(t+1)不会影响l(t)，所以</p>
<p><img alt="" src="../imgs/d42afef22699909665d77f174a25a5a2657300e2.png" /></p>
<p>因此有</p>
<p><img alt="" src="../imgs/0dbeebb42714ed3eb9b2f8c9084f8f86c090e285.png" /></p>
<p>这里如果能求得左边的式子(dL(t)/dh(t)) * (dh(t)/ds(t))，那么就可以由t+1到t来逐级反推dL(t)/ds(t)了</p>
<p>下面我们就来看怎么来计算左边的式子</p>
<p>因为我们神经元设计有：self.state.h = self.state.s * self.state.o也就是h(t) = s(t) * o(t)，那么dh(t)/ds(t) = o(t)，而因为dL(t)/dh(t)就是top_diff_h，所以有：</p>
<p><img alt="" src="../imgs/e777c34038cef50e9dedaccd9d10427dc7955e7b.png" /></p>
<p>所以得到</p>
<p><img alt="" src="../imgs/490527cea2624b7534b049d1889efcdebfe13393.png" /></p>
<p>这回可以回来看top_diff_is代码了，我自己读到这里的top和bottom的时候，没明白含义，所以特地咨询了代码原作者，他的解释是：Bottom means input to the layer, top means output of the layer. Caffe also uses this terminology.也就是说bottom表示神经网络层的输入，top表示神经网络层的输出，和caffe中概念一致</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">top_diff_is</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_diff_h</span><span class="p">,</span> <span class="n">top_diff_s</span><span class="p">):</span>
</pre></div>
</td></tr></table>

<p>首先看传递过来的两个参数：top_diff_h表示当前t时序的dL(t)/dh(t), top_diff_s表示t+1时序记忆单元的dL(t)/ds(t)</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>        <span class="n">ds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span> <span class="o">*</span> <span class="n">top_diff_h</span> <span class="o">+</span> <span class="n">top_diff_s</span>
        <span class="n">do</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span> <span class="o">*</span> <span class="n">top_diff_h</span>
        <span class="n">di</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">*</span> <span class="n">ds</span>
        <span class="n">dg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">*</span> <span class="n">ds</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s_prev</span> <span class="o">*</span> <span class="n">ds</span>
</pre></div>
</td></tr></table>

<p>这里面的前缀d表达的是误差L对某一项的导数(directive)</p>
<p>其中ds一行是在根据上面的公式dL(t)/ds(t)计算当前t时序的dL(t)/ds(t)</p>
<p>其中do一行是计算dL(t)/do(t)，因为h(t) = s(t) * o(t)，所以dh(t)/do(t) = s(t)，所以dL(t)/do(t) = (dL(t)/dh(t)) * (dh(t)/do(t)) = top_diff_h * s(t)</p>
<p>其中di一行是计算dL(t)/di(t)，考虑到</p>
<p><img alt="" src="../imgs/7581bad83c2d290187929c494d27b97b52073eed.png" /></p>
<p>换个符号表示就是</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">s</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>所以dL(t)/di(t) = (dL(t)/ds(t)) * (ds(t)/di(t)) = ds * g(t)</p>
<p>其中dg一行是计算dL(t)/dg(t)，同上有dL(t)/dg(t) = (dL(t)/ds(t)) * (ds(t)/dg(t)) = ds * i(t)</p>
<p>其中df一行是计算dL(t)/df(t)，同上有dL(t)/df(t) = (dL(t)/ds(t)) * (ds(t)/df(t)) = ds * s(t-1)</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">di_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">i</span> <span class="o">*</span> <span class="n">di</span>
<span class="n">df_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span> <span class="o">*</span> <span class="n">df</span>
<span class="n">do_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span>
<span class="n">dg_input</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">g</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dg</span>
</pre></div>
</td></tr></table>

<p>前三行用了sigmoid函数的导数，后一行用了tanh函数的导数。以第一行di_input为例，(1. - self.state.i) * self.state.i是sigmoid导数，表示当i神经元的输入发生单位变化时输出值有多大变化，那么再乘以di就表示当i神经元的输入发生单位变化时误差L(t)发生多大变化，也就是dL(t)/d i_input(t)，下面几个都同理</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wi_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">di_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wf_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">df_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wo_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">do_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wg_diff</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">dg_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bi_diff</span> <span class="o">+=</span> <span class="n">di_input</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bf_diff</span> <span class="o">+=</span> <span class="n">df_input</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bo_diff</span> <span class="o">+=</span> <span class="n">do_input</span>
<span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">bg_diff</span> <span class="o">+=</span> <span class="n">dg_input</span>
</pre></div>
</td></tr></table>

<p>这里的w<em>_diff是权重矩阵的误差，b</em>_diff是偏置的误差，用于做更新，这里面为什么是d*_input和xc的外积没看懂</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">dxc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xc</span><span class="p">)</span>
<span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wi</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">di_input</span><span class="p">)</span>
<span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wf</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">df_input</span><span class="p">)</span>
<span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wo</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">do_input</span><span class="p">)</span>
<span class="n">dxc</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">wg</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dg_input</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这里是在累加输入x的diff，因为x在四处起作用，所以四处的diff加和之后才算作x的diff</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_s</span> <span class="o">=</span> <span class="n">ds</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">f</span>
<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_x</span> <span class="o">=</span> <span class="n">dxc</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">x_dim</span><span class="p">]</span>
<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">bottom_diff_h</span> <span class="o">=</span> <span class="n">dxc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="o">.</span><span class="n">x_dim</span><span class="p">:]</span>
</pre></div>
</td></tr></table>

<p>这里bottom_diff_s是根据如下公式得出的推导关系，也就是在t-1时序上s的变化和t时序上s的变化时f倍的关系</p>
<p><img alt="" src="../imgs/7581bad83c2d290187929c494d27b97b52073eed.png" /></p>
<p>因为dxc是x和h横向合并出来的矩阵，所以分别取出两部分的diff信息就是bottom_diff_x和bottom_diff_h（这里面的bottom_diff_x代码里面没有真正作用）</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">x_list_clear</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">x_list_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">):</span>
        <span class="c1"># need to add new lstm node, create new state mem</span>
        <span class="n">lstm_state</span> <span class="o">=</span> <span class="n">LstmState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_param</span><span class="o">.</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_param</span><span class="o">.</span><span class="n">x_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LstmNode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lstm_param</span><span class="p">,</span> <span class="n">lstm_state</span><span class="p">))</span>

    <span class="c1"># get index of most recent x input</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_list</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># no recurrent inputs yet</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">bottom_data_is</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">s_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s</span>
        <span class="n">h_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">bottom_data_is</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_prev</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>这一部分是添加训练样本的过程，也就是输入x数据，那么整个执行过程如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">example_0</span><span class="p">():</span>
    <span class="c1"># learns to repeat simple sequence from random inputs</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># parameters for input data dimension and lstm cell count</span>
    <span class="n">mem_cell_ct</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">concat_len</span> <span class="o">=</span> <span class="n">x_dim</span> <span class="o">+</span> <span class="n">mem_cell_ct</span>
    <span class="n">lstm_param</span> <span class="o">=</span> <span class="n">LstmParam</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
    <span class="n">lstm_net</span> <span class="o">=</span> <span class="n">LstmNetwork</span><span class="p">(</span><span class="n">lstm_param</span><span class="p">)</span>
    <span class="n">y_list</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]</span>
    <span class="n">input_val_arr</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">x_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">y_list</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">cur_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">print</span> <span class="s2">&quot;cur iter: &quot;</span><span class="p">,</span> <span class="n">cur_iter</span>
        <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_list</span><span class="p">)):</span>
            <span class="n">lstm_net</span><span class="o">.</span><span class="n">x_list_add</span><span class="p">(</span><span class="n">input_val_arr</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
            <span class="k">print</span> <span class="s2">&quot;y_pred[</span><span class="si">%d</span><span class="s2">] : </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">lstm_net</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">lstm_net</span><span class="o">.</span><span class="n">y_list_is</span><span class="p">(</span><span class="n">y_list</span><span class="p">,</span> <span class="n">ToyLossLayer</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;loss: &quot;</span><span class="p">,</span> <span class="n">loss</span>
        <span class="n">lstm_param</span><span class="o">.</span><span class="n">apply_diff</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">lstm_net</span><span class="o">.</span><span class="n">x_list_clear</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<p>首先初始化LstmParam，指定记忆存储单元数为100，指定输入样本x维度是50，下面初始化一个LstmNetwork用于训练模型，然后生成了4组各50个随机数，并分别以[-0.5,0.2,0.1, -0.5]作为y值来训练，每次喂给50个随机数和一个y值，共迭代100次</p>
<p>这个测试样例最终执行效果自己来尝试吧</p>
<h3 id="lstm_1">利用lstm在输入一串连续质数时预估下一个质数<a class="headerlink" href="#lstm_1" title="Permanent link">&para;</a></h3>
<p>为了测试这个强大的lstm，我写了这样一个小测试，首先生成100以内质数，然后循环地拿出50个质数序列作为x，第51个质数作为y，这样拿出10个样本参与训练1w次，均方误差由一开始的0.17973最终达到了1.05172e-06，几乎完全正确，效果相当赞啊，程序如下：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">from</span> <span class="nn">lstm</span> <span class="kn">import</span> <span class="n">LstmParam</span><span class="p">,</span> <span class="n">LstmNetwork</span>

<span class="k">class</span> <span class="nc">ToyLossLayer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes square loss with first element of hidden layer array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">bottom_diff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">diff</span>

<span class="k">class</span> <span class="nc">Primes</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">primes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
            <span class="n">is_prime</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">is_prime</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">if</span> <span class="n">is_prime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">primes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">primes_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">primes</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_dim</span><span class="o">+</span><span class="n">y_dim</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">index</span> <span class="o">+</span> <span class="n">x_dim</span> <span class="o">+</span> <span class="n">y_dim</span><span class="p">):</span>
            <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">primes</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">primes_count</span><span class="p">]</span><span class="o">/</span><span class="mf">100.0</span>
        <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">example_0</span><span class="p">():</span>
    <span class="n">mem_cell_ct</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">concat_len</span> <span class="o">=</span> <span class="n">x_dim</span> <span class="o">+</span> <span class="n">mem_cell_ct</span>
    <span class="n">lstm_param</span> <span class="o">=</span> <span class="n">LstmParam</span><span class="p">(</span><span class="n">mem_cell_ct</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
    <span class="n">lstm_net</span> <span class="o">=</span> <span class="n">LstmNetwork</span><span class="p">(</span><span class="n">lstm_param</span><span class="p">)</span>

    <span class="n">primes</span> <span class="o">=</span> <span class="n">Primes</span><span class="p">()</span>
    <span class="n">x_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">primes</span><span class="o">.</span><span class="n">get_sample</span><span class="p">(</span><span class="n">x_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">x_dim</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="n">x_dim</span><span class="p">:</span><span class="n">x_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">cur_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">cur_iter</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">&quot;y_list=&quot;</span><span class="p">,</span> <span class="n">y_list</span>
        <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_list</span><span class="p">)):</span>
            <span class="n">lstm_net</span><span class="o">.</span><span class="n">x_list_add</span><span class="p">(</span><span class="n">x_list</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">cur_iter</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">print</span> <span class="s2">&quot;y_pred[</span><span class="si">%d</span><span class="s2">] : </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">lstm_net</span><span class="o">.</span><span class="n">lstm_node_list</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">lstm_net</span><span class="o">.</span><span class="n">y_list_is</span><span class="p">(</span><span class="n">y_list</span><span class="p">,</span> <span class="n">ToyLossLayer</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cur_iter</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s2">&quot;loss: &quot;</span><span class="p">,</span> <span class="n">loss</span>
        <span class="n">lstm_param</span><span class="o">.</span><span class="n">apply_diff</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">lstm_net</span><span class="o">.</span><span class="n">x_list_clear</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">example_0</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<p>最终的运行效果最后一次迭代结果输出如下(注：这里的质数列表我全都除以了100，因为这个代码训练的数据必须是小于1的数值)，可以看到预测基本上正确：</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">y_list</span><span class="o">=</span> <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.11</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.17</span><span class="p">,</span> <span class="mf">0.19</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.29</span><span class="p">]</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.019828</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.030286</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.049671</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.070302</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.109682</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.130395</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.169550</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.190424</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.229697</span>
<span class="n">y_pred</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="p">:</span> <span class="mf">0.290101</span>
<span class="n">loss</span><span class="p">:</span>  <span class="mf">1.05172051911e-06</span>
</pre></div>
</td></tr></table>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../32.word-vector/" title="三十二-用影视剧字幕语料库生成词向量" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                三十二-用影视剧字幕语料库生成词向量
              </span>
            </div>
          </a>
        
        
          <a href="../34.torch/" title="三十四-深度学习框架torch" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                三十四-深度学习框架torch
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            版权 &copy; 2016 - 2017 shareditor.com
          </div>
        
        powered by
        <a href="http://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.6cdc17f0.js"></script>
      
      <script>app.initialize({version:"0.17.2",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>